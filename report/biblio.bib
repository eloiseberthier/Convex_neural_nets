@article{bach2017breaking,
  title={Breaking the curse of dimensionality with convex neural networks},
  author={Bach, Francis},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={19},
  pages={1--53},
  year={2017}
}


@inproceedings{bengio2006convex,
  title={Convex neural networks},
  author={Bengio, Yoshua and Roux, Nicolas L and Vincent, Pascal and Delalleau, Olivier and Marcotte, Patrice},
  booktitle={Advances in neural information processing systems},
  pages={123--130},
  year={2006}
}


@article{chizat2018global,
  title={On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport},
  author={Chizat, Lenaic and Bach, Francis},
  journal={arXiv preprint arXiv:1805.09545},
  year={2018}
}

@article{zhang2016convexified,
  title={Convexified convolutional neural networks},
  author={Zhang, Yuchen and Liang, Percy and Wainwright, Martin J},
  journal={arXiv preprint arXiv:1609.01000},
  year={2016}
}

@inproceedings{haeffele2017global,
  title={Global optimality in neural network training},
  author={Haeffele, Benjamin D and Vidal, Ren{\'e}},
  year={2017}
}


@article{du2018agradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{du2018bgradient,
  title={Gradient Descent Finds Global Minima of Deep Neural Networks},
  author={Du, Simon S and Lee, Jason D and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  journal={arXiv preprint arXiv:1811.03804},
  year={2018}
}


@article{zou2018stochastic,
  title={Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:1811.08888},
  year={2018}
}

@inproceedings{visualloss,
  title={Visualizing the Loss Landscape of Neural Nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={Neural Information Processing Systems},
  year={2018}
}

@article{jacot2018neural,
  title={Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1806.07572},
  year={2018}
}

@inproceedings{le2007continuous,
  title={Continuous neural networks},
  author={Le Roux, Nicolas and Bengio, Yoshua},
  booktitle={Artificial Intelligence and Statistics},
  pages={404--411},
  year={2007}
}

@unpublished{chizat:hal-01945578,
  TITLE = {{A Note on Lazy Training in Supervised Differentiable Programming}},
  AUTHOR = {Chizat, Lenaic and Bach, Francis},
  URL = {https://hal.inria.fr/hal-01945578},
  NOTE = {working paper or preprint},
  YEAR = {2018},
  MONTH = Dec,
  KEYWORDS = {Neural network optimization ; Kernel regression ; Gradient flow},
  PDF = {https://hal.inria.fr/hal-01945578/file/chizatbach2018lazy.pdf},
  HAL_ID = {hal-01945578},
  HAL_VERSION = {v1},
}